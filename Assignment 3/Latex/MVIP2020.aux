\relax 
\citation{b3}
\citation{b7}
\citation{b2}
\citation{b1}
\citation{b6}
\citation{b7}
\citation{b1}
\citation{b5}
\citation{b4}
\citation{b1}
\citation{b2}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {II}Background and Related Works}{1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}YOLO (You Only Look Once)}{1}\protected@file@percent }
\citation{b3}
\citation{b4}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Processing images with YOLO is simple and straightforward. Our system (1) resizes the input image to 448 * 448, (2) runs a single convolutional network on the image, and (3) thresholds the resulting detections by the modelâ€™s confidence.}}{2}\protected@file@percent }
\newlabel{YOLODetectionSystem}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  SSD framework. (a) SSD only needs an input image and ground truth boxes for each object during training. In a convolutional fashion, we evaluate a small set of default boxes of different aspect ratios at each location in several feature maps with different scales (e.g. 8 * 8 and 4 * 4 in (b) and (c)). For each default box, we predict both the shape offsets and the confidences for all object categories ((c1; c2; ... ; cp)). At training time, we first match these default boxes to the ground truth boxes. For example, we have matched two default boxes with the cat and one with the dog, which are treated as positives and the rest as negatives. The model loss is a weighted sum between localization loss (e.g. Smooth L1) and confidence loss (e.g. Softmax). }}{2}\protected@file@percent }
\newlabel{YOLODetectionSystem}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}SSD (Single Shot MultiBox Detector)}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Multi-Scale Unified Object Detection (MSUOD) Framework}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Base Convolutional Neural Network (CNN)}{2}\protected@file@percent }
\newlabel{AA}{{\unhbox \voidb@x \hbox {III-A}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Grid-based Detection and Classification}{2}\protected@file@percent }
\newlabel{AA}{{\unhbox \voidb@x \hbox {III-B}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Multi-scale Feature Maps and Anchor Boxes}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Loss Function and Training}{2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental Results and Performance Evaluation}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Dataset and Evaluation Metrics}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Implementation Details}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-C}}Comparison with State-of-the-art Methods}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-D}}Ablation Study}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {V}Extensions and Future Research Directions}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Handling Occlusion and Rotation}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Network Architecture Improvements}{3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-C}}Attention Mechanisms}{3}\protected@file@percent }
\bibcite{b1}{1}
\bibcite{b2}{2}
\bibcite{b3}{3}
\bibcite{b4}{4}
\bibcite{b5}{5}
\bibcite{b6}{6}
\bibcite{b7}{7}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-D}}Multi-task Learning}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{References}{4}\protected@file@percent }
